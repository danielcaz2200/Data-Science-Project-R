billboard_tidy %>% group_by(artist) %>% summarise(artist_top_rank = min(rank)) %>% arrange(artist_top_rank)
#3i
billboard_tidy %>% group_by(track, artist) %>% summarise(weeks_on_bb = n()) %>% filter(weeks_on_bb == 1)
#3i
billboard_tidy %>% group_by(track) %>% summarise(weeks_on_bb = n()) %>% filter(weeks_on_bb == 1)
#3j
billboard_tidy %>% group_by(track, artist) %>% summarise(weeks_on_bb = n()) %>% filter(weeks_on_bb == 1)
#2b.
bands %>% inner_join(instruments, by=c(name="artist"))
billboard
library(tidyverse)
billboard
m <- lm(data=iris, formula=Petal.Length~Petal.Width)
m <- lm(data=iris, formula=Petal.Length~Petal.Width + Species)
m <- lm(data=iris, formula=Petal.Length~Petal.Width + Species)
m
library(tidyverse)
library(modelr)
m <- lm(data=iris, formula=Petal.Length~Petal.Width)
m <- lm(data=iris, formula=Petal.Length~Petal.Width + Species)
m
summary(m)
rm(iris)
m <- lm(data=iris, Petal.Length~Petal.Width + Species)
iris %>% add_predictions(m) %>% View()
iris %>% add_predictions(m) %>% View()
iris %>% add_predictions(m) %>% View()
m <- lm(data=iris, Petal.Length~Petal.Width + Species)
iris %>% add_predictions(m) %>% View()
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species))
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) +
geom_line(maping=aes(x=Petal.Width, y=pred, color=Species))
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) +
geom_line(mapping=aes(x=Petal.Width, y=pred, color=Species))
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) + geom_line(mapping=aes(x=Petal.Width, y=pred, color=Species))
iris <- iris %>% add_predictions(m)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) +
geom_line(mapping=aes(x=Petal.Width, y=pred, color=Species))
# linear regression equation for this
coef(m)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) +
geom_line(mapping=aes(x=Petal.Width, y=pred, color=Species))
m
m
levels(iris$Species)
rm(iris)
iris <- iris %>% mutate(PW2=Petal.Width^2)
View(iris)
lm(data=iris,Petal.Length~PW2)
iris <- iris %>% add_predictions()
mPW2 <- lm(formula=Petal.Length~PW2, data=iris)
gplot(data=iris) +
geom_point(mapping = aes(x=Petal.Width, y=Petal.Length)) + geom_line(mapping =
aes(x=Petal.Width, y=pred), color="red")
library(tidyverse)
ggplot(data=iris) +
geom_point(mapping = aes(x=Petal.Width, y=Petal.Length)) + geom_line(mapping =
aes(x=Petal.Width, y=pred), color="red")
mPW2 <- lm(formula=Petal.Length~PW2, data=iris)
iris <- iris %>% add_predictions(mPW2)
ggplot(data=iris) +
geom_point(mapping = aes(x=Petal.Width, y=Petal.Length)) + geom_line(mapping =
aes(x=Petal.Width, y=pred), color="red")
mPW2
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=resid))
library(modelr)
m <- lm(data=iris, Petal.Length~Petal.Width)
rm(iris)
iris %>% add_residuals(m) %>% View()
iris <- iris %>% add_residuals(m)
View(iris)
ggplot(data=iris) + geom_histogram(mapping=(x=resid))
rstandard(m)
# check for outliers
iris %>% mutate(rstd=rstandard(m)) %>% filter(rstd > 2 | rstd < -2) %>% View()
iris %>% mutate(rstd=rstandard(m)) %>% View()
# check for outliers
iris %>% mutate(rstd=rstandard(m)) %>% filter(rstd > 2 | rstd < -2) %>% View()
library(tidyverse)
library(modelr)
m <- lm(data=iris, Petal.Length~Petal.Width)
rm(iris)
iris %>% add_residuals(m) %>% View()
iris <- iris %>% add_residuals(m)
View(iris)
ggplot(data=iris) + geom_histogram(mapping=(x=resid))
# check for outliers
iris %>% mutate(rstd=rstandard(m)) %>% filter(rstd > 2 | rstd < -2) %>% View()
ggplot(data=iris) + geom_point(mapping=aes(x = Petal.Width, y = Petal.Length))+
geom_point(data = irisOutliers, mapping=aes(x = Petal.Width,
y = Petal.Length), color = "red")
ggplot(data=iris) + geom_point(mapping=aes(x = Petal.Width, y = Petal.Length))+
geom_point(data = irisOutliers, mapping=aes(x = Petal.Width,
y = Petal.Length), color = "red")
irisOutliers <- iris %>% mutate(rstd=rstandard(m)) %>% filter(rstd > 2 | rstd < -2)
ggplot(data=iris) + geom_point(mapping=aes(x = Petal.Width, y = Petal.Length))+
geom_point(data = irisOutliers, mapping=aes(x = Petal.Width,
y = Petal.Length), color = "red")
ggplot(data=iris) + geom_point(mapping=aes(x = Petal.Width, y = Petal.Length)) +
geom_abline(slope=mycf[2], intercept=mycf[1])
ggplot(data=iris) + geom_point(mapping=aes(x = Petal.Width, y = Petal.Length))+
geom_point(data = irisOutliers, mapping=aes(x = Petal.Width,
y = Petal.Length), color = "red") +
geom_abline(slope=mycf[2], intercept=mycf[1])
summary(m3)
m <- lm(data=iris, formula=Petal.Length~Petal.Width)
m <- lm(data=iris, formula=Petal.Length~Petal.Width + Species)
m
summary(m)
# Doing it in R
vec1-vec2
vec1 = c(5,0,-1)
vec2 = c(7,3,5)
# Doing it in R
vec1-vec2
# Doing it in R
sqrt(sum((c(5,0,-1) - c(7,3,5))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(6,0,2))^2))
sqrt(sum((c(5,0,1) - c(4,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(5,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(6,0,2))^2))
sqrt(sum((c(5,0,1) - c(4,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(5,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(6,0,2))^2))
sqrt(sum((c(5,0,1) - c(4,0,2))^2))
sqrt(sum((c(5,0,1) - c(5,1,3))^2))
sqrt(sum((c(5,0,1) - c(5,0,2))^2))
install.packages("class")
library(class)
# we need to normalize our vectors, so some units don't dominate others
(55000 - 1000) / (100000 - 1000)
# formula = x - min(x) / max(x) - min(x)
(x - min(x)) / (max(x) - min(x))
x <- c(16, 37, 50, 100)
# formula = x - min(x) / max(x) - min(x)
(x - min(x)) / (max(x) - min(x))
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(x)
View(iris)
iris %>% mutate(Sepal.Length.norm=normalize(Sepal.Length))
# knn 4/4/2023
library(tidyverse)
iris %>% mutate(Sepal.Length.norm=normalize(Sepal.Length))
iris %>% mutate(Sepal.Length.norm=normalize(Sepal.Length)) %>% View()
iris %>% mutate(Sepal.Length.norm=normalize(Sepal.Length)) %>%
mutate(Petal.Length.norm=normalize(Petal.Length)) %>% View()
# each col in table = a dimension
ncol(iris)
library(ggplot2)
library(tidyverse)
preg_v = c(2, 3, 2, 1, 2)
bp_v = c(74, 58, 58, 54, 70)
normalize(preg_v)
normalize(bp_v)
recent_new_d_smoothed_2wk
library(tidyverse)
library(ggplot2)
library(modelr)
library(lubridate)
library(dplyr)
library(grid)
library(gridExtra)
# ------------------------------
# PROJECT PART 1: DATA WRANGLING
# ------------------------------
# Read in both datasets
covid_data <- read_csv("owid-covid-data.csv")
setwd("C:/Users/danc2/Desktop/SPRING_2023/CPSC_375/375_Project")
library(tidyverse)
library(ggplot2)
library(modelr)
library(lubridate)
library(dplyr)
library(grid)
library(gridExtra)
# ------------------------------
# PROJECT PART 1: DATA WRANGLING
# ------------------------------
# Read in both datasets
covid_data <- read_csv("owid-covid-data.csv")
demographics <- read_csv("demographics.csv")
# Keep only the rows where country_code is 3 letters
# demographics <- demographics %>% filter(nchar(`Country Code`) == 3)
covid_data <- covid_data %>%
filter(nchar(iso_code) == 3, population >= 1000000)
# Select all columns BUT Series Name, we don't need this column
demographics <- demographics %>% select(-`Series Name`)
# Remove columns that should not be used for linear modeling
# In our case, cols dealting with death and excess mortality
covid_data <- covid_data %>%
select(-c(total_deaths, new_deaths, total_deaths_per_million,
new_deaths_per_million, new_deaths_smoothed_per_million))
covid_data <- covid_data %>%
select(-c(excess_mortality_cumulative_absolute, excess_mortality_cumulative,
excess_mortality, excess_mortality_cumulative_per_million))
# Add new column new_deaths_smoothed_2wk that has the same values as
# new_deaths_smoothed but two weeks ahead ...
# FIRST copy the data into a new dataframe
covid_data_copy <- covid_data
# Subtract 14 from date field, and rename the new_deaths_smoothed
# field in the copy
covid_data_copy <- covid_data_copy %>% mutate(date=as.Date(date)-14)
covid_data_copy <- covid_data_copy %>%
rename(new_deaths_smoothed_2wk = new_deaths_smoothed)
# Select specific columns to use on the join call
covid_data_copy <- covid_data_copy %>%
select(iso_code, date, new_deaths_smoothed_2wk)
# Join original covid data with copy
covid_data <- covid_data %>% inner_join(covid_data_copy)
# Tidy demographic table as needed
demographics <- demographics %>%
pivot_wider(names_from = `Series Code`, values_from = YR2015)
# Join the two tables together to get ready for linear regression
# demographics %>% inner_join(covid_data, by=c("Country Code"="iso_code"))
covid_data <- covid_data %>%
inner_join(demographics, by=c("iso_code" = "Country Code"))
covid_data
# -------------------------------
# PROJECT PART 2: LINEAR MODELING
# -------------------------------
# most recently available new deaths per day two weeks ahead
# 2023-03-15 is the last date before we get NAs
recent_new_d_smoothed_2wk <- covid_data %>%
filter(!is.na(new_deaths_smoothed_2wk)) %>% group_by(iso_code) %>% top_n(1, date)
recent_new_d_smoothed_2wk
recent_new_d_smoothed_2wk %>% view()
library(tidyverse)
library(ggplot2)
library(modelr)
library(lubridate)
library(dplyr)
library(grid)
library(gridExtra)
# ------------------------------
# PROJECT PART 1: DATA WRANGLING
# ------------------------------
# Read in both datasets
covid_data <- read_csv("owid-covid-data.csv")
demographics <- read_csv("demographics.csv")
# Keep only the rows where country_code is 3 letters
# demographics <- demographics %>% filter(nchar(`Country Code`) == 3)
covid_data <- covid_data %>%
filter(nchar(iso_code) == 3, population >= 1000000)
# Select all columns BUT Series Name, we don't need this column
demographics <- demographics %>% select(-`Series Name`)
# Remove columns that should not be used for linear modeling
# In our case, cols dealting with death and excess mortality
covid_data <- covid_data %>%
select(-c(total_deaths, new_deaths, total_deaths_per_million,
new_deaths_per_million, new_deaths_smoothed_per_million))
covid_data <- covid_data %>%
select(-c(excess_mortality_cumulative_absolute, excess_mortality_cumulative,
excess_mortality, excess_mortality_cumulative_per_million))
# Add new column new_deaths_smoothed_2wk that has the same values as
# new_deaths_smoothed but two weeks ahead ...
# FIRST copy the data into a new dataframe
covid_data_copy <- covid_data
# Subtract 14 from date field, and rename the new_deaths_smoothed
# field in the copy
covid_data_copy <- covid_data_copy %>% mutate(date=as.Date(date)-14)
covid_data_copy <- covid_data_copy %>%
rename(new_deaths_smoothed_2wk = new_deaths_smoothed)
# Select specific columns to use on the join call
covid_data_copy <- covid_data_copy %>%
select(iso_code, date, new_deaths_smoothed_2wk)
# Join original covid data with copy
covid_data <- covid_data %>% inner_join(covid_data_copy)
# Tidy demographic table as needed
demographics <- demographics %>%
pivot_wider(names_from = `Series Code`, values_from = YR2015)
# Join the two tables together to get ready for linear regression
# demographics %>% inner_join(covid_data, by=c("Country Code"="iso_code"))
covid_data <- covid_data %>%
inner_join(demographics, by=c("iso_code" = "Country Code"))
covid_data
# -------------------------------
# PROJECT PART 2: LINEAR MODELING
# -------------------------------
# most recently available new deaths per day two weeks ahead
# 2023-03-15 is the last date before we get NAs
recent_new_d_smoothed_2wk <- covid_data %>%
filter(!is.na(new_deaths_smoothed_2wk)) %>% group_by(iso_code) %>% top_n(1, date)
ggplot(data=recent_new_d_smoothed_2wk) +
geom_point(mapping=aes(x = new_cases_smoothed, y = new_deaths_smoothed_2wk))
# most recently available new deaths per day
recent_new_d_smoothed <- covid_data %>%
filter(!is.na(new_deaths_smoothed))%>% group_by(iso_code) %>% top_n(1, date)
ggplot(data=recent_new_d_smoothed) +
geom_point(mapping=aes(x = SP.URB.TOTL, y = new_deaths_smoothed))
library(tidyverse)
library(ggplot2)
library(modelr)
library(lubridate)
library(dplyr)
library(grid)
library(gridExtra)
# ------------------------------
# PROJECT PART 1: DATA WRANGLING
# ------------------------------
# Read in both datasets
covid_data <- read_csv("owid-covid-data.csv")
demographics <- read_csv("demographics.csv")
# Keep only the rows where country_code is 3 letters
# demographics <- demographics %>% filter(nchar(`Country Code`) == 3)
covid_data <- covid_data %>%
filter(nchar(iso_code) == 3, population >= 1000000)
# Select all columns BUT Series Name, we don't need this column
demographics <- demographics %>% select(-`Series Name`)
# Remove columns that should not be used for linear modeling
# In our case, cols dealting with death and excess mortality
covid_data <- covid_data %>%
select(-c(total_deaths, new_deaths, total_deaths_per_million,
new_deaths_per_million, new_deaths_smoothed_per_million))
covid_data <- covid_data %>%
select(-c(excess_mortality_cumulative_absolute, excess_mortality_cumulative,
excess_mortality, excess_mortality_cumulative_per_million))
# Add new column new_deaths_smoothed_2wk that has the same values as
# new_deaths_smoothed but two weeks ahead ...
# FIRST copy the data into a new dataframe
covid_data_copy <- covid_data
# Subtract 14 from date field, and rename the new_deaths_smoothed
# field in the copy
covid_data_copy <- covid_data_copy %>% mutate(date=as.Date(date)-14)
covid_data_copy <- covid_data_copy %>%
rename(new_deaths_smoothed_2wk = new_deaths_smoothed)
# Select specific columns to use on the join call
covid_data_copy <- covid_data_copy %>%
select(iso_code, date, new_deaths_smoothed_2wk)
# Join original covid data with copy
covid_data <- covid_data %>% inner_join(covid_data_copy)
# Tidy demographic table as needed
demographics <- demographics %>%
pivot_wider(names_from = `Series Code`, values_from = YR2015)
# Join the two tables together to get ready for linear regression
# demographics %>% inner_join(covid_data, by=c("Country Code"="iso_code"))
covid_data <- covid_data %>%
inner_join(demographics, by=c("iso_code" = "Country Code"))
covid_data
# -------------------------------
# PROJECT PART 2: LINEAR MODELING
# -------------------------------
# most recently available new deaths per day two weeks ahead
# 2023-03-15 is the last date before we get NAs
recent_new_d_smoothed_2wk <- covid_data %>%
filter(!is.na(new_deaths_smoothed_2wk)) %>% group_by(iso_code) %>% top_n(1, date)
ggplot(data=recent_new_d_smoothed_2wk) +
geom_point(mapping=aes(x = new_cases_smoothed, y = new_deaths_smoothed_2wk))
# most recently available new deaths per day
recent_new_d_smoothed <- covid_data %>%
filter(!is.na(new_deaths_smoothed))%>% group_by(iso_code) %>% top_n(1, date)
ggplot(data=recent_new_d_smoothed) +
geom_point(mapping=aes(x = SP.URB.TOTL, y = new_deaths_smoothed))
# 2b. Generate at least 3 transformed vars
# Description of variable and the R code transformations:
# Cardiovascular deaths is the cardiovascular disease death rate times total population
covid_data <- covid_data %>% mutate(cardiovasc_deaths = cardiovasc_death_rate * population)
# This is the population density per square mile/km, this helps if there is a non-linear
# relationship between population density and covid deaths
covid_data <- covid_data %>% mutate(population_density_squared = population_density^2)
# This is the rate of people living in urban areas, which is the urban pop total / pop total
covid_data <- covid_data %>% mutate(urban_pop_rate = (SP.URB.TOTL/SP.POP.TOTL)*100)
# 2c. Split your data set into train and test subsets
# Split data into train and test sets based on date
train_data <- covid_data %>% filter(year(date) == 2022)
test_data <- covid_data %>% filter(year(date) == 2023)
# filter out rows where new_deaths_smoothed_2wk is NA
test_data <- test_data %>% filter(!is.na(new_deaths_smoothed_2wk))
# Check the number of rows in each subset
nrow(train_data)
nrow(test_data)
# Run linear regression with at least 5 different combinations of predictor variables
model1 <- lm(data = train_data, formula = new_deaths_smoothed_2wk ~ new_cases_smoothed + gdp_per_capita +
diabetes_prevalence + icu_patients)
model2 <- lm(data = train_data, formula = new_deaths_smoothed_2wk ~ new_cases_smoothed + cardiovasc_deaths +
population_density_squared + hospital_beds_per_thousand)
model3 <- lm(data = train_data, formula = new_deaths_smoothed_2wk ~ new_cases_smoothed + urban_pop_rate +
human_development_index + hospital_beds_per_thousand)
model4 <- lm(data = train_data, formula = new_deaths_smoothed_2wk ~ new_cases_smoothed + life_expectancy +
gdp_per_capita + hospital_beds_per_thousand + human_development_index)
model5 <- lm(data = train_data, formula = new_deaths_smoothed_2wk ~ new_cases_smoothed + total_vaccinations_per_hundred +
diabetes_prevalence + cardiovasc_deaths + population_density_squared + urban_pop_rate)
# Print summary of each model to view coefficients and model statistics
summary(model1)
summary(model2)
summary(model3)
summary(model4)
summary(model5)
# prepare data for tabulation
r2_1 <- summary(model1)$adj.r.squared
r2_2 <- summary(model2)$adj.r.squared
r2_3 <- summary(model3)$adj.r.squared
r2_4 <- summary(model4)$adj.r.squared
r2_5 <- summary(model5)$adj.r.squared
# ---------------------------------
# PROJECT PART 3: EVALUATING MODELS
# ---------------------------------
# Calculating the Root Mean Squared Error (RMSE) over all days in 2023 and all countries
rmse_1 <- modelr::rmse(model=model1, data=test_data)
rmse_2 <- modelr::rmse(model=model2, data=test_data)
rmse_3 <- modelr::rmse(model=model3, data=test_data)
rmse_4 <- modelr::rmse(model=model4, data=test_data)
rmse_5 <- modelr::rmse(model=model5, data=test_data)
# Create RMSE and R2 Table
r2_values <- c(r2_1, r2_2, r2_3, r2_4, r2_5)
rmse_values <-  c(rmse_1, rmse_2, rmse_3, rmse_4, rmse_5)
r2_rmse <- data.frame(r2_values, rmse_values)
grid.newpage()
grid.table(r2_rmse)
# Print summary of each model to view coefficients and model statistics
summary(model1)
summary(model2)
summary(model3)
summary(model4)
summary(model5)
rmse_5
# prepare data for tabulation
r2_1 <- summary(model1)$adj.r.squared
r2_2 <- summary(model2)$adj.r.squared
r2_3 <- summary(model3)$adj.r.squared
r2_4 <- summary(model4)$adj.r.squared
r2_5 <- summary(model5)$adj.r.squared
# ---------------------------------
# PROJECT PART 3: EVALUATING MODELS
# ---------------------------------
# Calculating the Root Mean Squared Error (RMSE) over all days in 2023 and all countries
rmse_1 <- modelr::rmse(model=model1, data=test_data)
rmse_2 <- modelr::rmse(model=model2, data=test_data)
rmse_3 <- modelr::rmse(model=model3, data=test_data)
rmse_4 <- modelr::rmse(model=model4, data=test_data)
rmse_5 <- modelr::rmse(model=model5, data=test_data)
# Create RMSE and R2 Table
r2_values <- c(r2_1, r2_2, r2_3, r2_4, r2_5)
rmse_values <-  c(rmse_1, rmse_2, rmse_3, rmse_4, rmse_5)
r2_rmse <- data.frame(r2_values, rmse_values)
grid.newpage()
grid.table(r2_rmse)
# Note: NaN may happen bc certain parameters not feasible in certain countries
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>% top_n(20)
# View Best Model RMSE
rsme_by_country_top_20
# Create Best Model RMSE Table
grid.newpage()
grid.table(rsme_by_country_top_20)
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>% top_n(20, population)
# View Best Model RMSE
rsme_by_country_top_20
# Note: NaN may happen bc certain parameters not feasible in certain countries
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>% arrange(-population) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>%
top_n(20)
# View Best Model RMSE
rsme_by_country_top_20
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>%
top_n(20) %>% arrange(-population)
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>%
top_n(20, population)
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>%
top_n(20)
# View Best Model RMSE
rsme_by_country_top_20
rsme_by_country_top <- test_data %>%
group_by(iso_code)
rsme_by_country_top %>% arrange(desc(population))
rsme_by_country_top %>% arrange(population)
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>% arrange(desc(population)) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1))
rsme_by_country_top_20
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>% slice_max(order_by = population, n=20)
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1))
rsme_by_country_top_20
rsme_by_country_top_20 <- test_data %>% arrange(-population) %>%
group_by(iso_code) %>% slice_max(order_by = population, n=20)
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1))
# View Best Model RMSE
rsme_by_country_top_20
# Note: NaN may happen bc certain parameters not feasible in certain countries
rsme_by_country_top_20 <- test_data %>%
group_by(iso_code) %>%
summarise(rmse_country = modelr::rmse(data=cur_data(), model=model1)) %>% top_n(20)
# View Best Model RMSE
rsme_by_country_top_20
# Create Best Model RMSE Table
grid.newpage()
grid.table(rsme_by_country_top_20)
# Calculating the Root Mean Squared Error (RMSE) over all days in 2023 and all countries
rmse_1 <- modelr::rmse(model=model1, data=test_data)
rmse_1
# prepare data for tabulation
r2_1 <- summary(model1)$adj.r.squared
# Print summary of each model to view coefficients and model statistics
summary(model1)
